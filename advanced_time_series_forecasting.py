# -*- coding: utf-8 -*-
"""ADVANCED TIME SERIES FORECASTING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13iI9mlfGoOQU5qn4KNjY6JB_Ak24Kh5p
"""

# -- coding: utf-8 --
"""
Advanced Time Series Forecasting with Attention Mechanisms
Fully Corrected, Scalable, and Submission-Ready
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error


# =========================================================
# 1. SYNTHETIC MULTIVARIATE DATA GENERATION
# =========================================================

def generate_synthetic_data(n_steps=1000):
    t = np.arange(n_steps)

    data = pd.DataFrame({
        "feature_1": np.sin(0.02 * t) + 0.4*np.random.randn(n_steps),
        "feature_2": np.cos(0.015 * t) + 0.3*np.random.randn(n_steps),
        "feature_3": np.sin(0.01 * t + 1) + 0.25*np.random.randn(n_steps),
        "feature_4": 0.5 * np.sin(0.03 * t) + 0.2*np.random.randn(n_steps),
        "feature_5": 0.3 * np.cos(0.025 * t) + 0.2*np.random.randn(n_steps)
    })

    # Predict next value of feature_1
    data["target"] = data["feature_1"].shift(-1)
    data = data.dropna()

    return data


# =========================================================
# 2. DATASET + PREPROCESSING PIPELINE
# =========================================================

class TimeSeriesDataset(Dataset):
    def _init_(self, X, y, seq_len):
        self.X = X
        self.y = y
        self.seq_len = seq_len

    def _len_(self):
        return len(self.X) - self.seq_len

    def _getitem_(self, idx):
        seq_x = self.X[idx:idx+self.seq_len]
        y_val = self.y[idx+self.seq_len]
        return torch.tensor(seq_x, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)


def create_dataloaders(data, seq_len=24, batch_size=32):
    scaler_features = StandardScaler()
    scaler_target = StandardScaler()

    X = data.drop(columns=["target"]).values
    y = data["target"].values.reshape(-1, 1)

    X_scaled = scaler_features.fit_transform(X)
    y_scaled = scaler_target.fit_transform(y)

    split = int(len(X_scaled) * 0.8)

    train_dataset = TimeSeriesDataset(X_scaled[:split], y_scaled[:split], seq_len)
    test_dataset = TimeSeriesDataset(X_scaled[split:], y_scaled[split:], seq_len)

    return (
        DataLoader(train_dataset, batch_size=batch_size, shuffle=True),
        DataLoader(test_dataset, batch_size=batch_size, shuffle=False),
        scaler_features,
        scaler_target
    )


# =========================================================
# 3. ATTENTION MODULE + SEQ2SEQ MODEL
# =========================================================

class Attention(nn.Module):
    def _init_(self, hidden_dim):
        super()._init_()
        self.attn = nn.Linear(hidden_dim, hidden_dim)
        self.score = nn.Linear(hidden_dim, 1, bias=False)

    def forward(self, encoder_outputs):
        energy = torch.tanh(self.attn(encoder_outputs))
        weights = torch.softmax(self.score(energy), dim=1)
        context = torch.sum(weights * encoder_outputs, dim=1)
        return context, weights


class Seq2SeqAttention(nn.Module):
    def _init_(self, input_dim, hidden_dim):
        super()._init_()
        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.attention = Attention(hidden_dim)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        enc_out, _ = self.encoder(x)
        context, attn_w = self.attention(enc_out)
        return self.fc(context), attn_w


# =========================================================
# 4. BASELINE MODEL
# =========================================================

class BaselineLSTM(nn.Module):
    def _init_(self, input_dim, hidden_dim):
        super()._init_()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        last = out[:, -1, :]
        return self.fc(last)


# =========================================================
# 5. TRAINING LOOP
# =========================================================

def train_model(model, train_loader, epochs=20, lr=0.001):
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        model.train()
        losses = []

        for X, y in train_loader:
            optimizer.zero_grad()
            pred, *_ = (model(X) if isinstance(model, Seq2SeqAttention) else (model(X), None))
            loss = criterion(pred.squeeze(), y.squeeze())
            loss.backward()
            optimizer.step()
            losses.append(loss.item())

        print(f"Epoch {epoch+1}/{epochs} — Loss: {np.mean(losses):.4f}")

    return model


# =========================================================
# 6. EVALUATION (WITH INVERSE SCALING)
# =========================================================

def evaluate_model(model, test_loader, scaler_target):
    preds = []
    trues = []

    for X, y in test_loader:
        with torch.no_grad():
            pred, *_ = (model(X) if isinstance(model, Seq2SeqAttention) else (model(X), None))

        preds.append(pred.numpy())
        trues.append(y.numpy())

    preds = np.vstack(preds)
    trues = np.vstack(trues)

    preds = scaler_target.inverse_transform(preds)
    trues = scaler_target.inverse_transform(trues)

    mae = mean_absolute_error(trues, preds)
    rmse = np.sqrt(mean_squared_error(trues, preds))
    mape = np.mean(np.abs((trues - preds) / trues)) * 100

    return mae, rmse, mape


# =========================================================
# 7. MAIN EXECUTION
# =========================================================

def run_project():
    print("Generating data…")
    data = generate_synthetic_data()

    print("Creating dataset…")
    train_loader, test_loader, _, scaler_target = create_dataloaders(data)

    input_dim, hidden_dim = 5, 64

    print("\nTraining attention model…")
    att_model = Seq2SeqAttention(input_dim, hidden_dim)
    att_model = train_model(att_model, train_loader)

    print("\nExtracting attention weights…")
    sample_X, _ = next(iter(test_loader))
    _, att_weights = att_model(sample_X)
    attention_summary = att_weights.mean(dim=0).detach().numpy().flatten()

    print("\nTraining baseline LSTM…")
    base_model = BaselineLSTM(input_dim, hidden_dim)
    base_model = train_model(base_model, train_loader)

    print("\nEvaluating models…")
    att_mae, att_rmse, att_mape = evaluate_model(att_model, test_loader, scaler_target)
    base_mae, base_rmse, base_mape = evaluate_model(base_model, test_loader, scaler_target)

    print("\n=== FINAL RESULTS ===")
    print("Attention Model →", att_mae, att_rmse, att_mape)
    print("Baseline LSTM   →", base_mae, base_rmse, base_mape)

    print("\n=== ATTENTION INTERPRETATION (Deliverable 3) ===")
    print("Average attention weights over sequence:")
    print(attention_summary)


if _name_ == "_main_":
    run_project()